!pip install PyGithub
!pip install datasets
from github import Github
import re
from datasets import Dataset

# initialize pyGithub with Github token
g = Github("Your_Token_Here") #Create a token in github settings if you have committed codes in your repositories for training of the model, if not then use open tokens if available.
# specify the repository
repo = g.get_repo("openai/gym")

# function to extract Python functions from a script
def extract_function_from_code(code):
  pattern = re.compile(r"def\s+(\w+)\s*\(.*\):")
  functions = pattern.findall(code)
  return functions

# fetch Python files from the repository
python_files = []
contents = repo.get_contents("")
while contents:
  file_content = contents.pop(0)
  if file_content.type == "dir":
    contents.extend(repo.get_contents(file_content.path))
  elif file_content.path.endswith(".py"):
    python_files.append(file_content)

# extract functions and create dataset
data = {"code": [], "function_name": []}
for files in python_files:
  code = files.decoded_content.decode("utf-8")
  functions = extract_function_from_code(code)
  for function in functions:
    data["code"].append(code)
    data["function_name"].append(function)

dataset = Dataset.from_dict(data)
dataset.save_to_disk("code_generation_dataset")
print("Dataset saved to disk")

from datasets import load_from_disk
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments

tokanizer = AutoTokenizer.from_pretrained("Salesforce/codegen-350M-mono")
model = AutoModelForCausalLM.from_pretrained("Salesforce/codegen-350M-mono")

tokanizer.pad_token = tokanizer.eos_token

dataset = load_from_disk("code_generation_dataset")
dataset = dataset.train_test_split(test_size=0.1)

def preprocess_function(examples):
  return tokanizer(examples["code"], truncation=True, padding = "max_length")

tokenized_dataset = dataset.map(preprocess_function, batched=True)

training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=8,
    num_train_epochs = 1,
    save_steps = 10_000,
    save_total_limit = 2)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
)

trainer.train()

def generate_code(prompt, max_length=128):
  input_ids = tokanizer(prompt, return_tensors="pt").input_ids
  output = model.generate(input_ids, max_length=max_length)
  generated_code = tokanizer.decode(output[0], skip_special_tokens=True)
  return generated_code

prompt = "def merge_sort(arr:)"
generated_code = generate_code(prompt)
print("generated_code")
print(generated_code)
